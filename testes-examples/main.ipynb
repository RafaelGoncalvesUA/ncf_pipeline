{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Data\n",
    "First, we'll create a synthetic dataset. This data will be saved to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 1000\n",
    "num_features = 10\n",
    "\n",
    "X = np.random.rand(num_samples, num_features)\n",
    "y = X @ np.random.rand(num_features, 1) + np.random.normal(size=(num_samples, 1))\n",
    "\n",
    "# Create a DataFrame and save to CSV\n",
    "df = pd.DataFrame(np.hstack((X, y)), columns=[f\"feature_{i}\" for i in range(num_features)] + [\"label\"])\n",
    "df.to_csv(\"synthetic_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a PyTorch Model\n",
    "We’ll define a simple feedforward neural network in PyTorch for our regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Spark and Horovod\n",
    "Next, we’ll configure Spark and Horovod for distributed training. Make sure you have Spark and Horovod installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 12:35:22 WARN Utils: Your hostname, daniel-Yoga-Creator-7-15IMH05 resolves to a loopback address: 127.0.1.1; using 192.168.1.74 instead (on interface wlp0s20f3)\n",
      "24/06/10 12:35:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/10 12:35:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Exception in thread Thread-7 (run_spark):                         (0 + 12) / 12]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daniel/miniconda3/envs/caa/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/daniel/miniconda3/envs/caa/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/daniel/miniconda3/envs/caa/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/daniel/miniconda3/envs/caa/lib/python3.12/site-packages/horovod/spark/runner.py\", line 142, in run_spark\n",
      "    result = procs.mapPartitionsWithIndex(mapper).collect()\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daniel/miniconda3/envs/caa/lib/python3.12/site-packages/pyspark/rdd.py\", line 1833, in collect\n",
      "    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daniel/miniconda3/envs/caa/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daniel/miniconda3/envs/caa/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/daniel/miniconda3/envs/caa/lib/python3.12/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job 0 cancelled part of cancelled job group horovod.spark.run.0\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2731)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleJobGroupCancelled$4(DAGScheduler.scala:1198)\n",
      "\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:1197)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3016)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Neither MPI nor Gloo support has been built. Try reinstalling Horovod ensuring that either MPI is installed (MPI) or CMake is installed (Gloo).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHorovod_PyTorch_Example\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize Horovod\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m horovod\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mrun(spark)\n",
      "File \u001b[0;32m~/miniconda3/envs/caa/lib/python3.12/site-packages/horovod/spark/runner.py:290\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(fn, args, kwargs, num_proc, start_timeout, use_mpi, use_gloo, extra_mpi_args, env, stdout, stderr, verbose, nics, prefix_output_with_timestamp, executable)\u001b[0m\n\u001b[1;32m    286\u001b[0m     settings\u001b[38;5;241m.\u001b[39mhosts \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (host_hash, \u001b[38;5;28mlen\u001b[39m(driver\u001b[38;5;241m.\u001b[39mtask_host_hash_indices()[host_hash]))\n\u001b[1;32m    287\u001b[0m                               \u001b[38;5;28;01mfor\u001b[39;00m host_hash \u001b[38;5;129;01min\u001b[39;00m host_hashes)\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# Run the job\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m     _launch_job(use_mpi, use_gloo, settings, driver, env, stdout, stderr, executable)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# Terminate Spark job.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     spark_context\u001b[38;5;241m.\u001b[39mcancelJobGroup(spark_job_group)\n",
      "File \u001b[0;32m~/miniconda3/envs/caa/lib/python3.12/site-packages/horovod/spark/runner.py:155\u001b[0m, in \u001b[0;36m_launch_job\u001b[0;34m(use_mpi, use_gloo, settings, driver, env, stdout, stderr, executable)\u001b[0m\n\u001b[1;32m    153\u001b[0m nics \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mget_common_interfaces()\n\u001b[1;32m    154\u001b[0m executable \u001b[38;5;241m=\u001b[39m executable \u001b[38;5;129;01mor\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mexecutable\n\u001b[0;32m--> 155\u001b[0m run_controller(use_gloo, \u001b[38;5;28;01mlambda\u001b[39;00m: gloo_run(executable, settings, nics, driver, env, stdout, stderr),\n\u001b[1;32m    156\u001b[0m                use_mpi, \u001b[38;5;28;01mlambda\u001b[39;00m: mpi_run(executable, settings, nics, driver, env, stdout, stderr),\n\u001b[1;32m    157\u001b[0m                \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m                settings\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/miniconda3/envs/caa/lib/python3.12/site-packages/horovod/runner/launch.py:778\u001b[0m, in \u001b[0;36mrun_controller\u001b[0;34m(use_gloo, gloo_run, use_mpi, mpi_run, use_jsrun, js_run, verbosity)\u001b[0m\n\u001b[1;32m    776\u001b[0m     gloo_run()\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 778\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeither MPI nor Gloo support has been built. Try reinstalling Horovod ensuring that \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    779\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meither MPI is installed (MPI) or CMake is installed (Gloo).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Neither MPI nor Gloo support has been built. Try reinstalling Horovod ensuring that either MPI is installed (MPI) or CMake is installed (Gloo)."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 12:35:28 WARN TaskSetManager: Lost task 6.0 in stage 0.0 (TID 6) (daniel-Yoga-Creator-7-15IMH05.lan executor driver): TaskKilled (Stage cancelled: Job 0 cancelled part of cancelled job group horovod.spark.run.0)\n",
      "24/06/10 12:35:30 WARN PythonRunner: Incomplete task 11.0 in stage 0 (TID 11) interrupted: Attempting to kill Python Worker\n",
      "24/06/10 12:35:30 WARN PythonRunner: Incomplete task 2.0 in stage 0 (TID 2) interrupted: Attempting to kill Python Worker\n",
      "24/06/10 12:35:30 WARN PythonRunner: Incomplete task 5.0 in stage 0 (TID 5) interrupted: Attempting to kill Python Worker\n",
      "24/06/10 12:35:30 WARN PythonRunner: Incomplete task 10.0 in stage 0 (TID 10) interrupted: Attempting to kill Python Worker\n",
      "24/06/10 12:35:30 WARN TaskSetManager: Lost task 2.0 in stage 0.0 (TID 2) (daniel-Yoga-Creator-7-15IMH05.lan executor driver): TaskKilled (Stage cancelled: Job 0 cancelled part of cancelled job group horovod.spark.run.0)\n",
      "24/06/10 12:35:30 WARN TaskSetManager: Lost task 11.0 in stage 0.0 (TID 11) (daniel-Yoga-Creator-7-15IMH05.lan executor driver): TaskKilled (Stage cancelled: Job 0 cancelled part of cancelled job group horovod.spark.run.0)\n",
      "24/06/10 12:35:30 WARN TaskSetManager: Lost task 10.0 in stage 0.0 (TID 10) (daniel-Yoga-Creator-7-15IMH05.lan executor driver): TaskKilled (Stage cancelled: Job 0 cancelled part of cancelled job group horovod.spark.run.0)\n",
      "24/06/10 12:35:30 WARN PythonRunner: Incomplete task 1.0 in stage 0 (TID 1) interrupted: Attempting to kill Python Worker\n",
      "24/06/10 12:35:30 WARN TaskSetManager: Lost task 5.0 in stage 0.0 (TID 5) (daniel-Yoga-Creator-7-15IMH05.lan executor driver): TaskKilled (Stage cancelled: Job 0 cancelled part of cancelled job group horovod.spark.run.0)\n",
      "24/06/10 12:35:30 WARN PythonRunner: Incomplete task 4.0 in stage 0 (TID 4) interrupted: Attempting to kill Python Worker\n",
      "24/06/10 12:35:30 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1) (daniel-Yoga-Creator-7-15IMH05.lan executor driver): TaskKilled (Stage cancelled: Job 0 cancelled part of cancelled job group horovod.spark.run.0)\n",
      "24/06/10 12:35:30 WARN PythonRunner: Incomplete task 7.0 in stage 0 (TID 7) interrupted: Attempting to kill Python Worker\n",
      "24/06/10 12:35:30 WARN TaskSetManager: Lost task 4.0 in stage 0.0 (TID 4) (daniel-Yoga-Creator-7-15IMH05.lan executor driver): TaskKilled (Stage cancelled: Job 0 cancelled part of cancelled job group horovod.spark.run.0)\n",
      "24/06/10 12:35:30 WARN TaskSetManager: Lost task 7.0 in stage 0.0 (TID 7) (daniel-Yoga-Creator-7-15IMH05.lan executor driver): TaskKilled (Stage cancelled: Job 0 cancelled part of cancelled job group horovod.spark.run.0)\n",
      "24/06/10 12:35:30 WARN PythonRunner: Incomplete task 0.0 in stage 0 (TID 0) interrupted: Attempting to kill Python Worker\n",
      "24/06/10 12:35:30 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (daniel-Yoga-Creator-7-15IMH05.lan executor driver): TaskKilled (Stage cancelled: Job 0 cancelled part of cancelled job group horovod.spark.run.0)\n",
      "24/06/10 12:35:30 WARN PythonRunner: Incomplete task 9.0 in stage 0 (TID 9) interrupted: Attempting to kill Python Worker\n",
      "24/06/10 12:35:30 WARN TaskSetManager: Lost task 9.0 in stage 0.0 (TID 9) (daniel-Yoga-Creator-7-15IMH05.lan executor driver): TaskKilled (Stage cancelled: Job 0 cancelled part of cancelled job group horovod.spark.run.0)\n",
      "24/06/10 12:35:30 WARN PythonRunner: Incomplete task 8.0 in stage 0 (TID 8) interrupted: Attempting to kill Python Worker\n",
      "24/06/10 12:35:30 WARN TaskSetManager: Lost task 8.0 in stage 0.0 (TID 8) (daniel-Yoga-Creator-7-15IMH05.lan executor driver): TaskKilled (Stage cancelled: Job 0 cancelled part of cancelled job group horovod.spark.run.0)\n",
      "24/06/10 12:35:30 WARN PythonRunner: Incomplete task 3.0 in stage 0 (TID 3) interrupted: Attempting to kill Python Worker\n",
      "24/06/10 12:35:30 WARN TaskSetManager: Lost task 3.0 in stage 0.0 (TID 3) (daniel-Yoga-Creator-7-15IMH05.lan executor driver): TaskKilled (Stage cancelled: Job 0 cancelled part of cancelled job group horovod.spark.run.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import horovod.spark\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Horovod_PyTorch_Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize Horovod\n",
    "horovod.spark.run(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Spark\n",
    "Load the data into Spark DataFrame and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV into a Spark DataFrame\n",
    "df = spark.read.csv(\"synthetic_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model with Horovod\n",
    "Set up the Horovod Spark Estimator and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from horovod.spark.pytorch import SparkEstimator\n",
    "\n",
    "# Define the PyTorch model\n",
    "input_size = num_features\n",
    "model = SimpleModel(input_size)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Configure the SparkEstimator\n",
    "estimator = SparkEstimator(\n",
    "    num_proc=4,  # Number of processes (adjust based on your cluster)\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    input_shapes=[(input_size,)],  # Input shape for the model\n",
    "    label_shapes=[(1,)],  # Output shape (1D for regression)\n",
    "    feature_columns=[f\"feature_{i}\" for i in range(num_features)],  # Features from the dataset\n",
    "    label_columns=[\"label\"],  # Label column\n",
    "    batch_size=32,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "estimator.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and Save the Model\n",
    "Finally, evaluate the model and save it for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluation_results = estimator.evaluate(df)\n",
    "print(\"Evaluation Results:\", evaluation_results)\n",
    "\n",
    "# Save the model\n",
    "estimator.save(\"trained_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
